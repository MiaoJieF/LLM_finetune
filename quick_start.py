#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿå¼€å§‹è„šæœ¬
ä¸€é”®è¿è¡Œå®Œæ•´çš„LoRAå¾®è°ƒæµç¨‹
"""

import os
import sys
import subprocess
import json
from datetime import datetime

def run_command(command, description):
    """è¿è¡Œå‘½ä»¤å¹¶å¤„ç†é”™è¯¯"""
    print(f"\n{'='*50}")
    print(f"æ‰§è¡Œ: {description}")
    print(f"å‘½ä»¤: {command}")
    print(f"{'='*50}")
    
    try:
        # ä½¿ç”¨UTF-8ç¼–ç å¤„ç†è¾“å‡ºï¼Œé¿å…GBKè§£ç é”™è¯¯
        result = subprocess.run(
            command, 
            shell=True, 
            check=True, 
            capture_output=True, 
            text=True, 
            encoding='utf-8',
            errors='replace'  # é‡åˆ°æ— æ³•è§£ç çš„å­—ç¬¦æ—¶ç”¨æ›¿æ¢å­—ç¬¦ä»£æ›¿
        )
        print("âœ… æ‰§è¡ŒæˆåŠŸ!")
        if result.stdout:
            print("è¾“å‡º:", result.stdout)
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        if e.stderr:
            print("é”™è¯¯ä¿¡æ¯:", e.stderr)
        return False

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
    print("æ£€æŸ¥ä¾èµ–åŒ…...")
    
    required_packages = [
        "torch", "transformers", "peft", "datasets", 
        "numpy", "accelerate", "safetensors"
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
            print(f"âœ… {package}")
        except ImportError:
            print(f"âŒ {package}")
            missing_packages.append(package)
    
    if missing_packages:
        print(f"\nç¼ºå°‘ä¾èµ–åŒ…: {', '.join(missing_packages)}")
        print("è¯·è¿è¡Œ: pip install -r requirements.txt")
        return False
    
    return True

def check_model_path():
    """æ£€æŸ¥æ¨¡å‹è·¯å¾„"""
    model_path = "models/gemma3-1b"
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        print("è¯·ç¡®ä¿Gemmaæ¨¡å‹å·²ä¸‹è½½åˆ°æ­£ç¡®ä½ç½®")
        return False
    
    required_files = ["config.json", "tokenizer.json", "model.safetensors"]
    for file in required_files:
        if not os.path.exists(os.path.join(model_path, file)):
            print(f"âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶: {file}")
            return False
    
    print(f"âœ… æ¨¡å‹è·¯å¾„æ£€æŸ¥é€šè¿‡: {model_path}")
    return True

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Gemma LoRAå¾®è°ƒå¿«é€Ÿå¼€å§‹")
    print("="*60)
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        print("\nâŒ ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œè¯·å…ˆå®‰è£…ä¾èµ–åŒ…")
        return
    
    # æ£€æŸ¥æ¨¡å‹
    if not check_model_path():
        print("\nâŒ æ¨¡å‹æ£€æŸ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å®Œæ•´")
        return
    
    print("\nâœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹æ‰§è¡Œå¾®è°ƒæµç¨‹...")
    
    # æ­¥éª¤1: åˆ›å»ºæ•°æ®é›†
    print("\nğŸ“Š æ­¥éª¤1: åˆ›å»ºé“¶è¡Œé¢†åŸŸæ•°æ®é›†")
    if not run_command("python banking_dataset.py", "åˆ›å»ºè®­ç»ƒæ•°æ®é›†"):
        print("âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥")
        return
    
    # æ­¥éª¤2: å¼€å§‹LoRAå¾®è°ƒ
    print("\nğŸ”§ æ­¥éª¤2: å¼€å§‹LoRAå¾®è°ƒ")
    output_dir = f"outputs/lora_banking_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    finetune_cmd = f"""python lora_finetune.py \
        --model-path models/gemma3-1b \
        --output-dir {output_dir} \
        --epochs 20 \
        --batch-size 4 \
        --learning-rate 2e-4"""
    
    if not run_command(finetune_cmd, "LoRAå¾®è°ƒè®­ç»ƒ"):
        print("âŒ LoRAå¾®è°ƒå¤±è´¥")
        return
    
    # æ­¥éª¤3: è¿è¡Œå¯¹æ¯”æµ‹è¯•
    print("\nğŸ§ª æ­¥éª¤3: è¿è¡Œæ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    comparison_file = f"comparison_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    compare_cmd = f"""python compare_models.py \
        --peft-model-path {output_dir} \
        --output-file {comparison_file}"""
    
    if not run_command(compare_cmd, "æ¨¡å‹å¯¹æ¯”æµ‹è¯•"):
        print("âŒ å¯¹æ¯”æµ‹è¯•å¤±è´¥")
        return
    
    # æ­¥éª¤4: æ˜¾ç¤ºç»“æœæ‘˜è¦
    print("\nğŸ“ˆ æ­¥éª¤4: æ˜¾ç¤ºç»“æœæ‘˜è¦")
    try:
        with open(comparison_file, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        summary = results.get('summary', {})
        print(f"å¹³å‡æ”¹è¿›ç¨‹åº¦: {summary.get('average_improvement', 0):.3f}")
        print(f"æ­£å‘æ”¹è¿›: {summary.get('positive_improvements', 0)}")
        print(f"è´Ÿå‘æ”¹è¿›: {summary.get('negative_improvements', 0)}")
        print(f"æ— å˜åŒ–: {summary.get('neutral_improvements', 0)}")
        
    except Exception as e:
        print(f"è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
    
    # æ­¥éª¤5: æä¾›ä½¿ç”¨å»ºè®®
    print("\nğŸ‰ å¾®è°ƒå®Œæˆ!")
    print("="*60)
    print("æ¥ä¸‹æ¥ä½ å¯ä»¥:")
    print(f"1. æŸ¥çœ‹å¾®è°ƒæ¨¡å‹: {output_dir}")
    print(f"2. æŸ¥çœ‹å¯¹æ¯”ç»“æœ: {comparison_file}")
    print(f"3. ä½¿ç”¨å¾®è°ƒæ¨¡å‹å¯¹è¯:")
    print(f"   python chat_with_model.py --model-path {output_dir}")
    print("4. æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£: README_LoRA.md")
    
    print("\nâœ¨ å¿«é€Ÿå¼€å§‹å®Œæˆ!")

if __name__ == "__main__":
    main()
